import pandas as pd
import numpy as np
import glob
import tensorflow as tf
from tensorflow import keras
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
import pickle

# â”€â”€ 1. íŒŒì¼ë³„ë¡œ ë”°ë¡œ ë¡œë“œ â”€â”€
normal_files = sorted(glob.glob('normal_*.csv'))
defect_files = sorted(glob.glob('defect_*.csv'))

print(f"Normal íŒŒì¼: {normal_files}")
print(f"Defect íŒŒì¼: {defect_files}")

# â”€â”€ 2. Feature ì¶”ì¶œ í•¨ìˆ˜ â”€â”€
WINDOW = 50
STEP   = 25

def extract_features(window):
    ax  = window['ax'].values
    ay  = window['ay'].values
    az  = window['az'].values
    gx  = window['gx'].values
    gy  = window['gy'].values
    gz  = window['gz'].values
    cur = window['current'].values
    vol = window['voltage'].values
    pwr = window['power'].values
    accel_mag = np.sqrt(ax**2 + ay**2 + az**2)

    return {
        'ax_std':           ax.std(),
        'ay_std':           ay.std(),
        'az_std':           az.std(),
        'ax_p2p':           ax.max() - ax.min(),
        'ay_p2p':           ay.max() - ay.min(),
        'az_p2p':           az.max() - az.min(),
        'accel_mag_std':    accel_mag.std(),
        'accel_mag_mean':   accel_mag.mean(),
        'gx_std':           gx.std(),
        'gy_std':           gy.std(),
        'gz_std':           gz.std(),
        'gx_p2p':           gx.max() - gx.min(),
        'gy_p2p':           gy.max() - gy.min(),
        'gz_p2p':           gz.max() - gz.min(),
        'current_mean':     cur.mean(),
        'current_std':      cur.std(),
        'current_p2p':      cur.max() - cur.min(),
        'voltage_mean':     vol.mean(),
        'voltage_std':      vol.std(),
        'power_mean':       pwr.mean(),
        'current_over_019': (cur >= 0.19).sum() / len(cur),
    }

def file_to_windows(filepath, label):
    """íŒŒì¼ í•˜ë‚˜ â†’ ìœˆë„ìš° ë¦¬ìŠ¤íŠ¸ (ê²¹ì¹¨ ì—†ì´ ìˆœì„œëŒ€ë¡œ)"""
    df = pd.read_csv(filepath)
    features, labels = [], []
    for start in range(0, len(df) - WINDOW, STEP):
        window = df.iloc[start:start + WINDOW]
        features.append(extract_features(window))
        labels.append(label)
    return features, labels

# â”€â”€ 3. íŒŒì¼ ë‹¨ìœ„ë¡œ Train / Test ë¶„í•  â”€â”€
# normal: 2ê°œ â†’ 1ê°œëŠ” train, 1ê°œëŠ” test
# defect: 2ê°œ â†’ 1ê°œëŠ” train, 1ê°œëŠ” test

train_features, train_labels = [], []
test_features,  test_labels  = [], []

for i, f in enumerate(normal_files):
    feats, labs = file_to_windows(f, label=0)
    if i == 0:  # ì²«ë²ˆì§¸ íŒŒì¼ â†’ train
        train_features += feats
        train_labels   += labs
        print(f"[TRAIN] {f} â†’ {len(feats)}ê°œ ìœˆë„ìš°")
    else:        # ë‚˜ë¨¸ì§€ â†’ test
        test_features += feats
        test_labels   += labs
        print(f"[TEST]  {f} â†’ {len(feats)}ê°œ ìœˆë„ìš°")

for i, f in enumerate(defect_files):
    feats, labs = file_to_windows(f, label=1)
    if i == 0:
        train_features += feats
        train_labels   += labs
        print(f"[TRAIN] {f} â†’ {len(feats)}ê°œ ìœˆë„ìš°")
    else:
        test_features += feats
        test_labels   += labs
        print(f"[TEST]  {f} â†’ {len(feats)}ê°œ ìœˆë„ìš°")

X_train = pd.DataFrame(train_features).values.astype(np.float32)
y_train = np.array(train_labels, dtype=np.float32)
X_test  = pd.DataFrame(test_features).values.astype(np.float32)
y_test  = np.array(test_labels, dtype=np.float32)

print(f"\nTrain: {len(X_train)}ê°œ (ì •ìƒ {(y_train==0).sum()}, ë¹„ì •ìƒ {(y_train==1).sum()})")
print(f"Test:  {len(X_test)}ê°œ  (ì •ìƒ {(y_test==0).sum()},  ë¹„ì •ìƒ {(y_test==1).sum()})")

# â”€â”€ 4. ì •ê·œí™” (train ê¸°ì¤€ìœ¼ë¡œë§Œ fit!) â”€â”€
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)  # trainìœ¼ë¡œë§Œ fit
X_test  = scaler.transform(X_test)       # testëŠ” transformë§Œ!

pickle.dump(scaler, open('scaler.pkl', 'wb'))
np.save('scaler_mean.npy', scaler.mean_)
np.save('scaler_std.npy',  scaler.scale_)
print("Scaler ì €ì¥ ì™„ë£Œ")

# â”€â”€ 5. ëª¨ë¸ ì •ì˜ â”€â”€
model = keras.Sequential([
    keras.layers.Input(shape=(X_train.shape[1],)),
    keras.layers.Dense(64, activation='relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dropout(0.2),
    keras.layers.Dense(16, activation='relu'),
    keras.layers.Dense(1,  activation='sigmoid'),
])

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)
model.summary()

# â”€â”€ 6. í•™ìŠµ (validationì€ testì…‹ìœ¼ë¡œ ëª…ì‹œì  ì§€ì •) â”€â”€
history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    validation_data=(X_test, y_test),  # ëª…ì‹œì ìœ¼ë¡œ ë¶„ë¦¬ëœ íŒŒì¼ ì‚¬ìš©!
    callbacks=[
        keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),
        keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)
    ],
    verbose=1
)

# â”€â”€ 7. í‰ê°€ â”€â”€
loss, acc = model.evaluate(X_test, y_test, verbose=0)
# ì˜ˆì¸¡ score (0~1 í™•ë¥ )
y_score = model.predict(X_test).flatten()

# ìµœì¢… ì˜ˆì¸¡ label
y_pred = (y_score > 0.5).astype(int)

print(f"\ní…ŒìŠ¤íŠ¸ ì •í™•ë„: {acc:.4f}")

# ===== ğŸ”¥ ì¶”ê°€: ì¶”ë¡  ê²°ê³¼ CSV ì €ì¥ =====
result_df = pd.DataFrame({
    "true_label": y_test,
    "pred_label": y_pred,
    "anomaly_score": y_score
})

result_df.to_csv("inference_result.csv", index=False)
print("ì¶”ë¡  ê²°ê³¼ CSV ì €ì¥ ì™„ë£Œ â†’ inference_result.csv")



print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=['Normal','Defect']))
cm = confusion_matrix(y_test, y_pred)
print(f"ì •ìƒâ†’ì •ìƒ:         {cm[0][0]}")
print(f"ì •ìƒâ†’ë¹„ì •ìƒ(ì˜¤íƒ):  {cm[0][1]}")
print(f"ë¹„ì •ìƒâ†’ì •ìƒ(ë¯¸íƒ):  {cm[1][0]}")
print(f"ë¹„ì •ìƒâ†’ë¹„ì •ìƒ:     {cm[1][1]}")

# â”€â”€ 8. TFLite ë³€í™˜ â”€â”€
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
tflite_model = converter.convert()

with open('motor_anomaly.tflite', 'wb') as f:
    f.write(tflite_model)

print(f"\nTFLite ì €ì¥ ì™„ë£Œ: motor_anomaly.tflite")
print(f"ëª¨ë¸ í¬ê¸°: {len(tflite_model)/1024:.1f} KB")